You are improving aiderbot according to the information laid out in README.md

Look for anything missing, any tests that can be added, anything you can do to make it run unstoppably and controllably

And expand on the concepts used if needed

You should:
1. Make Live Aider Output respect Aider's control codes (like \c for cancel)
2. Ensure scrollback never exceeds 10,000 lines to prevent browser crashes
3. Prevent text duplication in both the Live Log and Diff Viewers
4. Keep the Live Log focused on current state and recent activity
5. Implement a working interrupt system that actually stops Aider
6. Respect changes to goal.prompt even after initial run if they're edited

You should make sure:
- All output follows proper formatting control codes
- Diffs are displayed with clear syntax highlighting
- The Live Log only shows relevant, non-duplicated activity
- Interrupt process sends proper signals and cleans up resources
- Goal changes trigger reinitialization of Aider with new instructions

# Current major goal
        # Check that the LLM evaluation prompt in the *second* iteration used the *updated* goal
        assert mock_get_llm.call_count == 2
        # The first argument to get_llm_response is the prompt
        first_eval_prompt = mock_get_llm.call_args_list[0].args[0]
        second_eval_prompt = mock_get_llm.call_args_list[1].args[0]

        # Check the goal embedded within the evaluation prompts
        logging.info(f"First eval prompt goal check:\n{first_eval_prompt}")
        # Check the main goal section at the start of the prompt
        assert first_eval_prompt.strip().startswith(f"Analyze the results of an automated code generation step in a test harness.\n\nCurrent Goal:\n{initial_content}"), \
               "First evaluation prompt did not start with the correct initial goal"
        logging.info(f"Second eval prompt goal check:\n{second_eval_prompt}")
        # Check the main goal section at the start of the prompt
>       assert second_eval_prompt.strip().startswith(f"Analyze the results of an automated code generation step in a test harness.\n\nCurrent Goal:\n{updated_content}"), \
               "Second evaluation prompt did not start with the correct updated goal"
E       AssertionError: Second evaluation prompt did not start with the correct updated goal
E       assert False
E        +  where False = <built-in method startswith of str object at 0x13a105600>('Analyze the results of an automated code generation step in a test harness.\n\nCurrent Goal:\nUpdated goal content!')
E        +    where <built-in method startswith of str object at 0x13a105600> = 'Analyze the results of an automated code generation step in a test harness.\n\nCurrent Goal:\nInitial goal content.\n\nIteration Context:\nThis is iteration 2 of maximum 3. Consider the progress made across iterations.\n\nConversation History (summary of last 5 exchanges):\nUSER: Initial goal content.\nASSISTANT: diff1\nSYSTEM: [System Event] Goal prompt reloaded from test_goal.prompt before evaluation.\nUSER: The previous attempt to achieve the goal needs improvement (Iteration 1 of 3):\n\nCurrent Goal:\n"Initial goal content."\n\nLast Code Changes:\n```diff\ndiff1\n```\n\nTest Results:\n```\npass1\n```\n\nEvaluation and Specific Suggestions for Improvement:\nsuggestion1\n\nYour Task:\n1. Carefully review the suggestions a...\nASSISTANT: diff2\n\nLast Code Changes (diff):\n```diff\ndiff2\n```\n\nTest Results:\nStatus: PASSED\n```\npass2\n```\n\nBased on the **current goal**, the conversation history, the latest code changes (diff), and the test results, evaluate the outcome.\n\nDetailed Evaluation Criteria:\n1. Goal Alignment: Do the changes directly address the requirements in the current goal?\n2. Test Results: Do all tests pass? If not, what specific issues are causing failures?\n3. Code Quality: Is the code well-structured, maintainable, and following best practices?\n4. Completeness: Does the implementation fully satisfy the goal, or are there missing elements?\n5. Edge Cases: Are there potential issues or edge cases not addressed?\n\nRespond using the following format:\n\nVerdict: [SUCCESS|RETRY|FAILURE]\nRationale: [Brief explanation of your verdict, considering the evaluation criteria]\nSuggestions: [Provide specific, actionable suggestions ONLY if the verdict is RETRY. Explain exactly what needs to be fixed and how. If SUCCESS or FAILURE, leave this blank.]'.startswith
E        +      where 'Analyze the results of an automated code generation step in a test harness.\n\nCurrent Goal:\nInitial goal content.\n\nIteration Context:\nThis is iteration 2 of maximum 3. Consider the progress made across iterations.\n\nConversation History (summary of last 5 exchanges):\nUSER: Initial goal content.\nASSISTANT: diff1\nSYSTEM: [System Event] Goal prompt reloaded from test_goal.prompt before evaluation.\nUSER: The previous attempt to achieve the goal needs improvement (Iteration 1 of 3):\n\nCurrent Goal:\n"Initial goal content."\n\nLast Code Changes:\n```diff\ndiff1\n```\n\nTest Results:\n```\npass1\n```\n\nEvaluation and Specific Suggestions for Improvement:\nsuggestion1\n\nYour Task:\n1. Carefully review the suggestions a...\nASSISTANT: diff2\n\nLast Code Changes (diff):\n```diff\ndiff2\n```\n\nTest Results:\nStatus: PASSED\n```\npass2\n```\n\nBased on the **current goal**, the conversation history, the latest code changes (diff), and the test results, evaluate the outcome.\n\nDetailed Evaluation Criteria:\n1. Goal Alignment: Do the changes directly address the requirements in the current goal?\n2. Test Results: Do all tests pass? If not, what specific issues are causing failures?\n3. Code Quality: Is the code well-structured, maintainable, and following best practices?\n4. Completeness: Does the implementation fully satisfy the goal, or are there missing elements?\n5. Edge Cases: Are there potential issues or edge cases not addressed?\n\nRespond using the following format:\n\nVerdict: [SUCCESS|RETRY|FAILURE]\nRationale: [Brief explanation of your verdict, considering the evaluation criteria]\nSuggestions: [Provide specific, actionable suggestions ONLY if the verdict is RETRY. Explain exactly what needs to be fixed and how. If SUCCESS or FAILURE, leave this blank.]' = <built-in method strip of str object at 0x13a105600>()
E        +        where <built-in method strip of str object at 0x13a105600> = 'Analyze the results of an automated code generation step in a test harness.\n\nCurrent Goal:\nInitial goal content.\n\nIteration Context:\nThis is iteration 2 of maximum 3. Consider the progress made across iterations.\n\nConversation History (summary of last 5 exchanges):\nUSER: Initial goal content.\nASSISTANT: diff1\nSYSTEM: [System Event] Goal prompt reloaded from test_goal.prompt before evaluation.\nUSER: The previous attempt to achieve the goal needs improvement (Iteration 1 of 3):\n\nCurrent Goal:\n"Initial goal content."\n\nLast Code Changes:\n```diff\ndiff1\n```\n\nTest Results:\n```\npass1\n```\n\nEvaluation and Specific Suggestions for Improvement:\nsuggestion1\n\nYour Task:\n1. Carefully review the suggestions a...\nASSISTANT: diff2\n\nLast Code Changes (diff):\n```diff\ndiff2\n```\n\nTest Results:\nStatus: PASSED\n```\npass2\n```\n\nBased on the **current goal**, the conversation history, the latest code changes (diff), and the test results, evaluate the outcome.\n\nDetailed Evaluation Criteria:\n1. Goal Alignment: Do the changes directly address the requirements in the current goal?\n2. Test Results: Do all tests pass? If not, what specific issues are causing failures?\n3. Code Quality: Is the code well-structured, maintainable, and following best practices?\n4. Completeness: Does the implementation fully satisfy the goal, or are there missing elements?\n5. Edge Cases: Are there potential issues or edge cases not addressed?\n\nRespond using the following format:\n\nVerdict: [SUCCESS|RETRY|FAILURE]\nRationale: [Brief explanation of your verdict, considering the evaluation criteria]\nSuggestions: [Provide specific, actionable suggestions ONLY if the verdict is RETRY. Explain exactly what needs to be fixed and how. If SUCCESS or FAILURE, leave this blank.]'.strip

tests/test_harness.py:406: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    root:harness.py:851 Failed final goal reload for evaluation:
ERROR    root:harness.py:519 Error checking goal prompt file hash:
ERROR    root:harness.py:825 Error checking goal file before evaluation:
ERROR    root:harness.py:851 Failed final goal reload for evaluation:
=========================== short test summary info ============================
FAILED tests/test_harness.py::test_reloaded_goal_prompt_is_used - AssertionError: Second evaluation prompt did not start with the correct upd...
============ 1 failed, 58 passed, 6 skipped, 2 deselected in 7.11s =============
Fix these errors
