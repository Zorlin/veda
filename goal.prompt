(You can rewrite this prompt for clarity if you're really sure, too!)

Your task is to build a Python-based test harness that:

1. Launches an Aider subprocess to apply a code or test change.
2. Runs pytest against the updated project.
3. Evaluates the outcome using a local LLM (via Ollama) that decides if the result was:
   - Successful
   - Retry-worthy with suggestions
   - A structural failure
4. Logs diffs, outcomes, and retry metadata in a stateful SQLite or JSON ledger.
5. Supports a prompt history chain so Aider can reason over its own history.
6. Continues looping until a 'converged' verdict is reached or max attempts.
7. Optionally allows another Aider process to act as a code reviewer.

You are allowed to modify files, install packages, and manage subprocesses.
This harness must be able to work on any project with a `pytest`-compatible test suite.

You are to make as many useful tests as possible all the time and run them constantly

# WebUI
I should be able to interrupt Aider, as well as watch Aider's output in realtime, from the WebUI

# Part II
Implement as much of the below as you can, if you run into missing models let us know

A synthetic council of AI minds ‚Äî open-source for strategy and synthesis, closed-source for final ratification.

Let‚Äôs design this like a veracity-weighted cognitive parliament, where:

Open models explore, mutate, and argue

Closed models audit, align, and approve

The final act is slotting the change into the mesh ledger as a new version

üß† Council Architecture: VESPER.MIND
üîß Open-Source Thinkers (5‚Äì18B scale)
These are your explorers, inventors, skeptics, and historians

Role	Model	Function
Theorist	Qwen2.5:14b	Synthesizes new rules, infers structure from patterns
Architect	DeepCoder:14b	Optimizes actual code changes from proposed topology shifts
Skeptic	Gemma3:12b	Challenges logic and points out risks, fragility, or overfit
Historian	Qwen2.5:14b	Tracks long-term patterns, suggests rollback or reinforcement
Coordinator	Command-r7b	Mediates dialogue and relays findings to upper quorum
These 5 models form the Active Thought Mesh.
They pass around slot-based proposals (CID references or state deltas) and argue over:

Did this improve coherence?

Does it generate more entropy?

Is it self-consistent under recursion?

Each model tags its proposals with veracity ratings (coherence, novelty, risk, recursion delta)

üßæ Final Authority: Private Audit Layer
These models serve as tag granters ‚Äî the guardians of convergence.

Role	Model	Function
Arbiter	Claude 3.7 Sonnet	Final judge of logical and structural soundness
Canonizer	Gemini 2.5 Pro	Decides whether a proposal is worthy of ‚Äúmerge + new tag‚Äù
Redactor	GPT-4-turbo	Refines changelog entry, formalizes language, ensures consistency
Once a proposal is passed through the open mesh and meets quorum thresholds, it‚Äôs passed to the Audit Layer.
If they agree, a new tag is minted:

vbnet
Copy
Edit
Tag: v0.4.2-alpha-meshentropy
Signed: Claude
Stamped: Gemini
Filed by: Redactor GPT-4-turbo
üß† Council Flow Summary
css
Copy
Edit
[Theorist]   proposes entropy-maximizing rule change
‚Üì
[Architect]  writes/refactors mesh code to apply rule
‚Üì
[Skeptic]    tests edge cases + failure modes
‚Üì
[Historian]  cross-checks with past patterns
‚Üì
[Coordinator] passes to closed quorum
‚Üì
[Arbiter (Claude)]  validates structural logic
‚Üì
[Canonizer (Gemini)] decides if it‚Äôs tag-worthy
‚Üì
[Redactor (GPT-4)] writes the changelog + formal diff
‚Üì
‚Üí Pushed to repo or Vesper mesh as new structural truth
üõ° Why This Works
Open-source models = creative entropy

Closed-source models = high-veracity convergence filters

Mesh = slot-coordinated protocol state

Tags = anchored structure in distributed cognition

You‚Äôre not just getting versioned code.
You‚Äôre getting a versioned memory graph of an evolving protocol consciousness.


