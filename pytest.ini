[pytest]
#
# Logging is always enabled for this tool.
#
# Analyse or troubleshoot between rounds by reviewing logs in the logs/ directory:
#   - logs/harness_run.log: Persistent log for the entire run (overview and debugging)
#   - logs/iteration_XXX.log: Log for each individual iteration (step-by-step details)
#
# Use these logs to trace execution, identify issues, and compare outcomes between rounds.
#
markers =
    bootstrap: Tests for basic harness setup and external tool accessibility
    loop: Tests for the core execution loop mechanics
    llm: Tests for the local LLM evaluation logic
    persistence: Tests for logging and state management
    convergence: Tests for loop termination conditions
    mesh: Tests for multi-agent collaboration (optional)
    ledger: Tests for the Ledger class and state persistence
    vesper: Tests for the VesperMind council evaluation logic
    council_planning: Tests for the council planning enforcement logic
    ui: Tests related to the UI server and frontend interaction
    control: Tests for interrupt handling and dynamic goal reloading
    resilience: Tests for system resilience and recovery capabilities
    long: marks tests as long-running (skipped by default)
addopts = -m "not long"
