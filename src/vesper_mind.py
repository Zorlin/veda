import logging
from typing import Dict, Any, List, Optional, Tuple
import json
import os
from pathlib import Path

from .llm_interaction import get_llm_response
from .ledger import Ledger

# Configure logging
logger = logging.getLogger(__name__)

class VesperMind:
    """
    Implements the VESPER.MIND council architecture for code evaluation and improvement.
    
    The council consists of:
    - Open-Source Thinkers: Theorist, Architect, Skeptic, Historian, Coordinator
    - Final Authority: Arbiter, Canonizer, Redactor
    """
    
    def __init__(
        self, 
        config: Dict[str, Any],
        ledger: Ledger,
        work_dir: Path
    ):
        self.config = config
        self.ledger = ledger
        self.work_dir = work_dir
        
        # Define council members and their roles
        self.open_source_council = {
            "theorist": {
                "model": config.get("theorist_model", "qwen:14b"),
                "description": "Synthesizes new rules, infers structure from patterns"
            },
            "architect": {
                "model": config.get("architect_model", "deepseek-coder:16b"),
                "description": "Optimizes actual code changes from proposed topology shifts"
            },
            "skeptic": {
                "model": config.get("skeptic_model", "gemma:7b"),
                "description": "Challenges logic and points out risks, fragility, or overfit"
            },
            "historian": {
                "model": config.get("historian_model", "yi:34b"),
                "description": "Tracks long-term patterns, suggests rollback or reinforcement"
            },
            "coordinator": {
                "model": config.get("coordinator_model", "command-r-plus"),
                "description": "Mediates dialogue and relays findings to upper quorum"
            }
        }
        
        self.closed_source_council = {
            "arbiter": {
                "model": config.get("arbiter_model", "claude-3-sonnet"),
                "description": "Final judge of logical and structural soundness"
            },
            "canonizer": {
                "model": config.get("canonizer_model", "gemini-pro"),
                "description": "Decides whether a proposal is worthy of merge + new tag"
            },
            "redactor": {
                "model": config.get("redactor_model", "gpt-4-turbo"),
                "description": "Refines changelog entry, formalizes language, ensures consistency"
            }
        }
        
        # Check which models are available
        self.available_models = self._check_available_models()
        logger.info(f"VESPER.MIND initialized with {len(self.available_models)} available models")

    def _check_available_models(self) -> List[str]:
        """Check which models are available in the Ollama installation."""
        available_models = []
        
        # For open-source models, check if they're available in Ollama
        for role, details in self.open_source_council.items():
            model_name = details["model"]
            try:
                # Use a simple prompt to test if the model is available
                test_prompt = "Hello"
                response = get_llm_response(
                    test_prompt,
                    {"ollama_model": model_name},
                    history=None,
                    system_prompt="Respond with 'OK' if you can see this message."
                )
                available_models.append(model_name)
                logger.info(f"Model {model_name} for role {role} is available")
            except Exception as e:
                logger.warning(f"Model {model_name} for role {role} is not available: {e}")
        
        # For closed-source models, we'll assume they're not available through Ollama
        # and will be handled through API calls or other means
        
        return available_models

    def evaluate_iteration(
        self,
        run_id: int,
        iteration_id: int,
        initial_goal: str,
        aider_diff: str,
        pytest_output: str,
        pytest_passed: bool,
        history: List[Dict[str, str]]
    ) -> Tuple[str, str, Dict[str, Any]]:
        """
        Run the VESPER.MIND council evaluation on an iteration.
        
        Args:
            run_id: The run ID.
            iteration_id: The iteration ID.
            initial_goal: The initial goal prompt.
            aider_diff: The diff generated by Aider.
            pytest_output: The output from pytest.
            pytest_passed: Whether pytest passed.
            history: The conversation history.
            
        Returns:
            Tuple containing:
            - verdict: "SUCCESS", "RETRY", or "FAILURE"
            - suggestions: Suggestions for improvement if verdict is "RETRY"
            - council_results: Dictionary with detailed council evaluations
        """
        logger.info(f"Running VESPER.MIND council evaluation for iteration {iteration_id}")
        
        # Initialize council results
        council_results = {
            "open_source": {},
            "closed_source": {},
            "final_verdict": None,
            "final_suggestions": None
        }
        
        # Run open-source council evaluations
        for role, details in self.open_source_council.items():
            model_name = details["model"]
            if model_name in self.available_models:
                logger.info(f"Running {role} evaluation with {model_name}")
                evaluation = self._run_open_source_evaluation(
                    role, model_name, initial_goal, aider_diff, pytest_output, pytest_passed, history
                )
                council_results["open_source"][role] = evaluation
                
                # Store in ledger
                self.ledger.add_council_evaluation(
                    iteration_id, model_name, role, evaluation["evaluation"], evaluation["score"]
                )
            else:
                logger.warning(f"Skipping {role} evaluation as {model_name} is not available")
        
        # Synthesize open-source evaluations for the closed-source council
        open_source_summary = self._synthesize_open_source_evaluations(council_results["open_source"])
        
        # Run closed-source council evaluations if available
        # For now, we'll use the default LLM as a stand-in for closed-source models
        default_model = self.config.get("ollama_model", "gemma3:12b")
        
        for role, details in self.closed_source_council.items():
            model_name = details["model"]
            logger.info(f"Running {role} evaluation (using {default_model} as stand-in)")
            evaluation = self._run_closed_source_evaluation(
                role, default_model, initial_goal, aider_diff, pytest_output, 
                pytest_passed, history, open_source_summary
            )
            council_results["closed_source"][role] = evaluation
            
            # Store in ledger
            self.ledger.add_council_evaluation(
                iteration_id, f"{default_model} (as {model_name})", role, 
                evaluation["evaluation"], evaluation["score"]
            )
        
        # Determine final verdict and suggestions
        verdict, suggestions = self._determine_final_verdict(council_results)
        council_results["final_verdict"] = verdict
        council_results["final_suggestions"] = suggestions
        
        logger.info(f"VESPER.MIND council verdict: {verdict}")
        return verdict, suggestions, council_results

    def _run_open_source_evaluation(
        self,
        role: str,
        model_name: str,
        initial_goal: str,
        aider_diff: str,
        pytest_output: str,
        pytest_passed: bool,
        history: List[Dict[str, str]]
    ) -> Dict[str, Any]:
        """Run evaluation with an open-source council member."""
        role_description = self.open_source_council[role]["description"]
        
        # Create role-specific system prompt
        system_prompt = f"""You are the {role.title()} in the VESPER.MIND council.
Your role: {role_description}

Analyze the code changes and test results based on your specific perspective.
Focus on your area of expertise and provide a detailed evaluation.

Respond in JSON format with the following structure:
{{
    "evaluation": "Your detailed evaluation...",
    "score": A score from 0.0 to 1.0 representing your assessment,
    "concerns": ["List any specific concerns"],
    "recommendations": ["List specific recommendations"]
}}"""

        # Create role-specific evaluation prompt
        prompt = f"""
As the {role.title()}, evaluate the following code changes:

Initial Goal:
{initial_goal}

Code Changes (diff):
```diff
{aider_diff if aider_diff else "[No changes made]"}
```

Test Results:
Passed: {pytest_passed}
```
{pytest_output}
```

Based on your role as {role.title()} ({role_description}), provide your evaluation.
"""

        # Get response from the model
        try:
            response = get_llm_response(
                prompt,
                {"ollama_model": model_name},
                history=None,
                system_prompt=system_prompt
            )
            
            # Parse JSON response
            try:
                # Extract JSON if it's wrapped in markdown code blocks
                if "```json" in response:
                    json_start = response.find("```json") + 7
                    json_end = response.find("```", json_start)
                    json_str = response[json_start:json_end].strip()
                    evaluation = json.loads(json_str)
                elif "```" in response:
                    json_start = response.find("```") + 3
                    json_end = response.find("```", json_start)
                    json_str = response[json_start:json_end].strip()
                    evaluation = json.loads(json_str)
                else:
                    evaluation = json.loads(response)
                
                # Ensure all required fields are present
                if "evaluation" not in evaluation:
                    evaluation["evaluation"] = "No evaluation provided"
                if "score" not in evaluation:
                    evaluation["score"] = 0.5
                if "concerns" not in evaluation:
                    evaluation["concerns"] = []
                if "recommendations" not in evaluation:
                    evaluation["recommendations"] = []
                
                return evaluation
            except json.JSONDecodeError:
                logger.warning(f"Could not parse JSON response from {role} evaluation")
                return {
                    "evaluation": response,
                    "score": 0.5,
                    "concerns": ["Response format error"],
                    "recommendations": ["Review manually"]
                }
        except Exception as e:
            logger.error(f"Error running {role} evaluation: {e}")
            return {
                "evaluation": f"Error: {str(e)}",
                "score": 0.0,
                "concerns": ["Evaluation failed"],
                "recommendations": ["Try with a different model"]
            }

    def _run_closed_source_evaluation(
        self,
        role: str,
        model_name: str,
        initial_goal: str,
        aider_diff: str,
        pytest_output: str,
        pytest_passed: bool,
        history: List[Dict[str, str]],
        open_source_summary: str
    ) -> Dict[str, Any]:
        """Run evaluation with a closed-source council member (or stand-in)."""
        role_description = self.closed_source_council[role]["description"]
        
        # Create role-specific system prompt
        system_prompt = f"""You are the {role.title()} in the VESPER.MIND council.
Your role: {role_description}

You are part of the Final Authority layer that reviews the evaluations from the Open-Source Thinkers.
Your task is to make a final judgment based on your expertise and the input from other council members.

Respond in JSON format with the following structure:
{{
    "evaluation": "Your detailed evaluation...",
    "score": A score from 0.0 to 1.0 representing your assessment,
    "verdict": "SUCCESS", "RETRY", or "FAILURE",
    "rationale": "Explanation for your verdict",
    "suggestions": "Specific suggestions for improvement if verdict is RETRY"
}}"""

        # Create role-specific evaluation prompt
        prompt = f"""
As the {role.title()}, review the following code changes and open-source council evaluations:

Initial Goal:
{initial_goal}

Code Changes (diff):
```diff
{aider_diff if aider_diff else "[No changes made]"}
```

Test Results:
Passed: {pytest_passed}
```
{pytest_output}
```

Open-Source Council Evaluations:
{open_source_summary}

Based on your role as {role.title()} ({role_description}), provide your final evaluation.
"""

        # Get response from the model
        try:
            response = get_llm_response(
                prompt,
                {"ollama_model": model_name},
                history=None,
                system_prompt=system_prompt
            )
            
            # Parse JSON response
            try:
                # Extract JSON if it's wrapped in markdown code blocks
                if "```json" in response:
                    json_start = response.find("```json") + 7
                    json_end = response.find("```", json_start)
                    json_str = response[json_start:json_end].strip()
                    evaluation = json.loads(json_str)
                elif "```" in response:
                    json_start = response.find("```") + 3
                    json_end = response.find("```", json_start)
                    json_str = response[json_start:json_end].strip()
                    evaluation = json.loads(json_str)
                else:
                    evaluation = json.loads(response)
                
                # Ensure all required fields are present
                if "evaluation" not in evaluation:
                    evaluation["evaluation"] = "No evaluation provided"
                if "score" not in evaluation:
                    evaluation["score"] = 0.5
                if "verdict" not in evaluation:
                    evaluation["verdict"] = "RETRY"
                if "rationale" not in evaluation:
                    evaluation["rationale"] = "No rationale provided"
                if "suggestions" not in evaluation:
                    evaluation["suggestions"] = "No suggestions provided"
                
                return evaluation
            except json.JSONDecodeError:
                logger.warning(f"Could not parse JSON response from {role} evaluation")
                return {
                    "evaluation": response,
                    "score": 0.5,
                    "verdict": "RETRY",
                    "rationale": "Response format error",
                    "suggestions": "Review manually"
                }
        except Exception as e:
            logger.error(f"Error running {role} evaluation: {e}")
            return {
                "evaluation": f"Error: {str(e)}",
                "score": 0.0,
                "verdict": "RETRY",
                "rationale": "Evaluation failed",
                "suggestions": "Try with a different model"
            }

    def _synthesize_open_source_evaluations(self, evaluations: Dict[str, Dict[str, Any]]) -> str:
        """Synthesize open-source evaluations into a summary for closed-source council."""
        if not evaluations:
            return "No open-source evaluations available."
        
        summary = []
        for role, evaluation in evaluations.items():
            summary.append(f"## {role.title()} Evaluation")
            summary.append(f"Score: {evaluation.get('score', 'N/A')}")
            summary.append(f"Evaluation: {evaluation.get('evaluation', 'No evaluation provided')}")
            
            concerns = evaluation.get('concerns', [])
            if concerns:
                summary.append("Concerns:")
                for concern in concerns:
                    summary.append(f"- {concern}")
            
            recommendations = evaluation.get('recommendations', [])
            if recommendations:
                summary.append("Recommendations:")
                for recommendation in recommendations:
                    summary.append(f"- {recommendation}")
            
            summary.append("")  # Empty line between evaluations
        
        return "\n".join(summary)

    def _determine_final_verdict(self, council_results: Dict[str, Any]) -> Tuple[str, str]:
        """Determine the final verdict and suggestions based on all council evaluations."""
        # Start with the Arbiter's verdict if available
        if "arbiter" in council_results["closed_source"]:
            arbiter_verdict = council_results["closed_source"]["arbiter"].get("verdict", "RETRY")
            arbiter_suggestions = council_results["closed_source"]["arbiter"].get("suggestions", "")
            
            # If Canonizer agrees with SUCCESS, it's a success
            if arbiter_verdict == "SUCCESS" and "canonizer" in council_results["closed_source"]:
                canonizer_verdict = council_results["closed_source"]["canonizer"].get("verdict", "")
                if canonizer_verdict == "SUCCESS":
                    # Get refined suggestions from Redactor if available
                    if "redactor" in council_results["closed_source"]:
                        redactor_eval = council_results["closed_source"]["redactor"].get("evaluation", "")
                        return "SUCCESS", redactor_eval
                    return "SUCCESS", "All council members agree on success."
            
            # If Arbiter says FAILURE, it's a failure
            if arbiter_verdict == "FAILURE":
                return "FAILURE", arbiter_suggestions
            
            # Otherwise, it's a RETRY with suggestions
            suggestions = arbiter_suggestions
            if "redactor" in council_results["closed_source"]:
                redactor_suggestions = council_results["closed_source"]["redactor"].get("suggestions", "")
                if redactor_suggestions:
                    suggestions = redactor_suggestions
            
            return "RETRY", suggestions
        
        # If no closed-source evaluations, use open-source consensus
        open_source_scores = [eval.get("score", 0.5) for eval in council_results["open_source"].values()]
        if open_source_scores:
            avg_score = sum(open_source_scores) / len(open_source_scores)
            
            if avg_score >= 0.8 and pytest_passed:
                return "SUCCESS", "Open-source council consensus indicates success."
            elif avg_score < 0.3:
                return "FAILURE", "Open-source council consensus indicates fundamental issues."
            else:
                # Collect recommendations from all open-source members
                all_recommendations = []
                for role, eval in council_results["open_source"].items():
                    recommendations = eval.get("recommendations", [])
                    for rec in recommendations:
                        all_recommendations.append(f"{role.title()}: {rec}")
                
                suggestions = "\n".join(all_recommendations) if all_recommendations else "Review and improve based on test results."
                return "RETRY", suggestions
        
        # Default fallback
        return "RETRY", "Insufficient council evaluations. Review and improve based on test results."

    def generate_changelog(self, run_id: int, iteration_id: int, verdict: str) -> str:
        """
        Generate a changelog entry for a successful iteration.
        
        Args:
            run_id: The run ID.
            iteration_id: The iteration ID.
            verdict: The final verdict.
            
        Returns:
            Changelog entry as a string.
        """
        if verdict != "SUCCESS":
            return ""
        
        # Use the Redactor's evaluation if available
        if "redactor" in self.closed_source_council:
            model_name = self.config.get("ollama_model", "gemma3:12b")
            
            # Get run summary from ledger
            run_summary = self.ledger.get_run_summary(run_id)
            
            # Find the iteration
            iteration = None
            for iter_data in run_summary.get("iterations", []):
                if iter_data["iteration_id"] == iteration_id:
                    iteration = iter_data
                    break
            
            if not iteration:
                return "Could not generate changelog: iteration not found."
            
            # Create changelog prompt
            prompt = f"""
Generate a formal changelog entry for the following successful code changes:

Initial Goal:
{run_summary.get("initial_goal", "Unknown goal")}

Code Changes (diff):
```diff
{iteration.get("aider_diff", "[No changes recorded]")}
```

The changes were successful and all tests passed.

Generate a concise, professional changelog entry that:
1. Summarizes what was changed
2. Explains why it was changed
3. Notes any potential impacts

Format the changelog as a Markdown entry with a version tag.
"""

            system_prompt = """You are the Redactor in the VESPER.MIND council.
Your role is to refine changelog entries, formalize language, and ensure consistency.
Generate a professional, concise changelog entry in Markdown format."""

            try:
                changelog = get_llm_response(
                    prompt,
                    {"ollama_model": model_name},
                    history=None,
                    system_prompt=system_prompt
                )
                
                # Add version tag if not present
                if "# v" not in changelog and "## v" not in changelog:
                    iteration_num = iteration.get("iteration_number", 0)
                    changelog = f"## v0.1.{iteration_num}-vesper\n\n{changelog}"
                
                return changelog
            except Exception as e:
                logger.error(f"Error generating changelog: {e}")
                return f"Error generating changelog: {e}"
        
        # Simple fallback changelog
        return f"## Successful Changes\n\nIteration {iteration_id} completed successfully."
